#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Royal Research Automation System - Main Orchestration

This script orchestrates the complete YouTube research automation workflow:
1. Collect data from all sources (Google Trends, Twitter, Reddit, YouTube, News)
2. Generate AI-powered topic recommendations using Claude
3. Produce comprehensive research reports
4. Save results to files or send via email

Author: Royal Research Automation
Created: 2026-01-18
"""

import os
import json
import time
import argparse
from datetime import datetime, timezone
from typing import Dict, Tuple, List

import config
from utils.logger import logger
from collectors.google_trends import GoogleTrendsCollector
from collectors.twitter_client import TwitterCollector
from collectors.reddit_scraper import RedditScraper
from collectors.youtube_client import YouTubeCompetitorTracker
from collectors.news_aggregator import NewsAggregator
from generators.claude_client import ClaudeTopicGenerator


def run_research() -> Dict:
    """
    Run the complete research automation workflow.

    Returns:
        Dictionary containing complete research report with:
            - success: bool
            - timestamp: str
            - sources_collected: list
            - sources_failed: list
            - raw_data: dict
            - formatted_data: dict
            - claude_result: dict
            - cost_breakdown: dict
            - executive_summary: str
    """
    logger.info("üöÄ Starting Royal Research Automation...")

    workflow_start = time.time()
    report = {
        "success": False,
        "timestamp": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC"),
        "sources_collected": [],
        "sources_failed": [],
        "raw_data": {},
        "formatted_data": {},
        "claude_result": {},
        "cost_breakdown": {
            "claude_api": 0.0,
            "youtube_api_quota": 0,
            "newsapi_requests": 0
        },
        "executive_summary": "",
        "executive_summary": "",
        "runtime_seconds": 0,
        "performance": {
            "collection_times": {},
            "sources_collected_count": 0
        }
    }

    # Tracking collection times
    collection_times = {}

    # =======================================================================
    # PHASE 1: INITIALIZE ALL COLLECTORS
    # =======================================================================
    logger.info("üìä PHASE 1: Initializing Data Collectors")
    logger.info("=" * 60)

    try:
        # Load settings
        from utils.settings_manager import CompetitorManager, KeywordManager
        
        comp_mgr = CompetitorManager()
        kw_mgr = KeywordManager()
        
        active_competitors = comp_mgr.get_active()
        active_keywords = kw_mgr.get_active()
        
        logger.info(f"üì∫ Loaded {len(active_competitors)} competitors")
        logger.info(f"üîë Loaded {len(active_keywords)} keywords")
        
        # Validate settings
        if not active_competitors:
            logger.warning("‚ö†Ô∏è  No active competitors found!")
        
        if not active_keywords:
            logger.error("‚ùå No active keywords found! Cannot proceed.")
            return report

        # Google Trends
        trends_collector = GoogleTrendsCollector(keywords=active_keywords)
        logger.info("‚úÖ Google Trends collector initialized")

        # Twitter
        twitter_collector = TwitterCollector(
            bearer_token=config.TWITTER_BEARER_TOKEN,
            min_engagement=1000
        )
        logger.info("‚úÖ Twitter collector initialized")

        # Reddit
        reddit_collector = RedditScraper()
        logger.info("‚úÖ Reddit scraper initialized")

        # YouTube - only if competitors exist
        youtube_collector = None
        if active_competitors:
            competitor_dict = {c['name']: c['channel_id'] for c in active_competitors}
            youtube_collector = YouTubeCompetitorTracker(
                api_key=config.YOUTUBE_API_KEY,
                competitor_channels=competitor_dict
            )
            logger.info("‚úÖ YouTube tracker initialized")
        else:
            logger.info("‚ÑπÔ∏è  YouTube tracker skipped (no active competitors)")

        # News
        news_collector = NewsAggregator(
            news_api_key=config.NEWSAPI_KEY,
            rss_feeds={}  # No hardcoded feeds, rely on NewsAPI or user config
        )
        logger.info("‚úÖ News aggregator initialized")

        logger.info("‚úÖ All collectors initialized successfully\n")

    except Exception as e:
        logger.error(f"‚ùå Failed to initialize collectors: {e}")
        raise

    # =======================================================================
    # PHASE 2: COLLECT DATA FROM ALL SOURCES
    # =======================================================================
    logger.info("üìä PHASE 2: Data Collection")
    logger.info("=" * 60)

    # =======================================================================
    # PHASE 2: COLLECT DATA FROM ALL SOURCES (PARALLEL)
    # =======================================================================
    logger.info("üìä PHASE 2: Data Collection (Parallel)")
    logger.info("=" * 60)

    import concurrent.futures

    # Define wrapper functions for each collector to handle exceptions individually
    def collect_trends():
        try:
            logger.info("Collecting Google Trends data...")
            start_t = time.time()
            data = trends_collector.collect()
            elapsed = time.time() - start_t
            return "google_trends", data, elapsed, None
        except Exception as e:
            return "google_trends", None, 0, str(e)

    def collect_twitter():
        try:
            logger.info("Collecting Twitter data...")
            start_t = time.time()
            data = twitter_collector.collect(keywords=active_keywords, hours_back=24)
            elapsed = time.time() - start_t
            return "twitter", data, elapsed, None
        except Exception as e:
            return "twitter", None, 0, str(e)

    def collect_reddit():
        try:
            logger.info("Collecting Reddit data...")
            start_t = time.time()
            # Pass keywords to trigger search mode if subreddit is default
            data = reddit_collector.collect(hours_back=24, keywords=active_keywords)
            elapsed = time.time() - start_t
            return "reddit", data, elapsed, None
        except Exception as e:
            return "reddit", None, 0, str(e)

    def collect_youtube():
        if not youtube_collector:
            return "youtube", "No competitors configured", 0, None
        try:
            logger.info("Collecting YouTube competitor data...")
            start_t = time.time()
            data = youtube_collector.collect(hours_back=48, videos_per_channel=5)
            elapsed = time.time() - start_t
            return "youtube", data, elapsed, None
        except Exception as e:
            return "youtube", None, 0, str(e)

    def collect_news():
        try:
            logger.info("Collecting News data...")
            start_t = time.time()
            data = news_collector.collect(
                keywords=active_keywords,
                hours_back=24,
                max_articles=15
            )
            elapsed = time.time() - start_t
            return "news", data, elapsed, None
        except Exception as e:
            return "news", None, 0, str(e)

    # Execute in parallel
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = [
            executor.submit(collect_trends),
            executor.submit(collect_twitter),
            executor.submit(collect_reddit),
            executor.submit(collect_youtube),
            executor.submit(collect_news)
        ]
        
        for future in concurrent.futures.as_completed(futures):
            source, data, elapsed, error = future.result()
            
            if error:
                collection_times[source] = 0
                logger.error(f"‚ùå {source.replace('_', ' ').title()}: Failed - {error}")
                report["sources_failed"].append(source)
                report["raw_data"][source] = {"error": error}
            else:
                collection_times[source] = elapsed
                report["raw_data"][source] = data
                
                # Special handling for "skipped" youtube
                if source == "youtube" and isinstance(data, str) and "No competitors" in data:
                     logger.info("‚ÑπÔ∏è  YouTube: Skipped (no competitors)")
                else:
                    report["sources_collected"].append(source)
                    logger.info(f"‚úÖ {source.replace('_', ' ').title()}: Success ({elapsed:.2f}s)")
                    
                    # Updates for costs
                    if source == "youtube":
                        report["cost_breakdown"]["youtube_api_quota"] = data.get("quota_used", 0)
                    elif source == "news":
                        report["cost_breakdown"]["newsapi_requests"] = data.get("newsapi_requests", 0)

    # Check if at least one collector succeeded
    if not report["sources_collected"]:
        error_msg = "‚ùå All data collectors failed. Cannot proceed to AI generation."
        logger.error(error_msg)
        raise Exception(error_msg)

    logger.info(f"\n‚úÖ Data collection complete: {len(report['sources_collected'])}/5 sources successful\n")

    # =======================================================================
    # PHASE 3: FORMAT DATA FOR CLAUDE
    # =======================================================================
    logger.info("üìä PHASE 3: Formatting Data for AI Analysis")
    logger.info("=" * 60)

    research_data = {}

    # Format Google Trends
    if "google_trends" in report["sources_collected"]:
        research_data["google_trends"] = trends_collector.format_for_prompt(report["raw_data"]["google_trends"])
        report["formatted_data"]["google_trends"] = research_data["google_trends"]
        logger.info("‚úÖ Google Trends data formatted")
    else:
        research_data["google_trends"] = "No data available - collection failed"
        report["formatted_data"]["google_trends"] = research_data["google_trends"]

    # Format Twitter
    if "twitter" in report["sources_collected"]:
        research_data["twitter"] = twitter_collector.format_for_prompt(report["raw_data"]["twitter"])
        report["formatted_data"]["twitter"] = research_data["twitter"]
        logger.info("‚úÖ Twitter data formatted")
    else:
        research_data["twitter"] = "No data available - collection failed"
        report["formatted_data"]["twitter"] = research_data["twitter"]

    # Format Reddit
    if "reddit" in report["sources_collected"]:
        research_data["reddit"] = reddit_collector.format_for_prompt(report["raw_data"]["reddit"])
        report["formatted_data"]["reddit"] = research_data["reddit"]
        logger.info("‚úÖ Reddit data formatted")
    else:
        research_data["reddit"] = "No data available - collection failed"
        report["formatted_data"]["reddit"] = research_data["reddit"]

    # Format YouTube
    if "youtube" in report["sources_collected"]:
        research_data["youtube"] = youtube_collector.format_for_prompt(report["raw_data"]["youtube"])
        research_data["youtube_raw"] = report["raw_data"]["youtube"]  # Pass raw data for title generator
        report["formatted_data"]["youtube"] = research_data["youtube"]
        logger.info("‚úÖ YouTube data formatted")
    else:
        research_data["youtube"] = "No data available - collection failed"
        report["formatted_data"]["youtube"] = research_data["youtube"]

    # Format News
    if "news" in report["sources_collected"]:
        research_data["news"] = news_collector.format_for_prompt(report["raw_data"]["news"])
        report["formatted_data"]["news"] = research_data["news"]
        logger.info("‚úÖ News data formatted")
    else:
        research_data["news"] = "No data available - collection failed"
        report["formatted_data"]["news"] = research_data["news"]

    logger.info("‚úÖ All data formatted for AI analysis\n")

    # =======================================================================
    # PHASE 4: GENERATE TOPICS WITH CLAUDE AI
    # =======================================================================
    logger.info("ü§ñ PHASE 4: AI Topic Generation")
    logger.info("=" * 60)

    try:
        # Initialize Claude
        claude_generator = ClaudeTopicGenerator(
            api_key=config.ANTHROPIC_API_KEY,
            model="claude-3-5-haiku-20241022"  # Using Haiku for speed and cost
        )

        # Generate topics
        claude_result = claude_generator.generate_topics(research_data)
        report["claude_result"] = claude_result
        report["cost_breakdown"]["claude_api"] = claude_result.get("cost_estimate", 0.0)

        logger.info(f"‚úÖ Generated {len(claude_result.get('topic_recommendations', []))} topic recommendations")
        logger.info(f"üí∞ Claude API cost: ${claude_result.get('cost_estimate', 0.0):.4f}\n")

    except Exception as e:
        logger.error(f"‚ùå Claude AI generation failed: {e}")
        logger.error("üíæ Saving raw data for manual review...")
        raise

    # =======================================================================
    # PHASE 5: CREATE EXECUTIVE SUMMARY
    # =======================================================================
    topics = claude_result.get("topic_recommendations", [])
    themes = claude_result.get("trending_themes", [])

    top_topic = topics[0] if topics else {"title": "N/A", "publishing_priority": 0}
    themes_str = ", ".join(themes[:3]) if themes else "N/A"

    report["executive_summary"] = (
        f"Generated {len(topics)} topics from {len(report['sources_collected'])} data sources. "
        f"Top opportunity: {top_topic.get('title', 'N/A')} "
        f"({top_topic.get('publishing_priority', 0)}/10 priority). "
        f"Trending themes: {themes_str}. "
        f"Cost: ${report['cost_breakdown']['claude_api']:.4f}."
    )

    # =======================================================================
    # PHASE 6: FINALIZE REPORT
    # =======================================================================
    report["success"] = True
    report["runtime_seconds"] = time.time() - workflow_start
    report["performance"]["collection_times"] = collection_times
    report["performance"]["sources_collected_count"] = len(report["sources_collected"])

    logger.info("‚úÖ Research automation complete!")
    logger.info(f"‚è±Ô∏è  Runtime: {report['runtime_seconds']:.1f} seconds\n")

    return report


def save_report(report: Dict, output_dir: str = "data/research_reports") -> Tuple[str, str]:
    """
    Save research report to JSON and HTML files.

    Args:
        report: Research report dictionary from run_research()
        output_dir: Directory to save reports (default: data/research_reports)

    Returns:
        Tuple of (json_path, html_path)
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)

    # Generate filename timestamp
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d_%H-%M")

    # Save JSON report
    json_filename = f"research_report_{timestamp}.json"
    json_path = os.path.join(output_dir, json_filename)

    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, default=str)

    logger.info(f"üíæ JSON report saved: {json_path}")

    # Save HTML report
    html_filename = f"research_report_{timestamp}.html"
    html_path = os.path.join(output_dir, html_filename)

    # Generate HTML from Claude result
    if report.get("claude_result"):
        claude_generator = ClaudeTopicGenerator(api_key=config.ANTHROPIC_API_KEY)
        html_content = claude_generator.format_for_email(report["claude_result"])

        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)

        logger.info(f"üíæ HTML report saved: {html_path}")
    else:
        html_path = None
        logger.warning("‚ö†Ô∏è  HTML report not generated (no Claude result)")

    return json_path, html_path


def print_summary(report: Dict) -> None:
    """
    Print beautiful console summary of research results.

    Args:
        report: Research report dictionary from run_research()
    """
    print("\n" + "=" * 70)
    print("üìä RESEARCH AUTOMATION SUMMARY")
    print("=" * 70)

    # Timestamp
    print(f"\nüïê Completed: {report['timestamp']}")
    print(f"‚è±Ô∏è  Runtime: {report['runtime_seconds']:.1f} seconds")

    # Data sources status
    print(f"\nüìÅ DATA SOURCES ({len(report['sources_collected'])}/5 successful)")
    print("-" * 70)
    all_sources = ["google_trends", "twitter", "reddit", "youtube", "news"]
    for source in all_sources:
        if source in report['sources_collected']:
            print(f"  ‚úÖ {source.replace('_', ' ').title()}")
        else:
            print(f"  ‚ùå {source.replace('_', ' ').title()} (failed)")

    # Claude AI results
    claude_result = report.get("claude_result", {})
    topics = claude_result.get("topic_recommendations", [])
    themes = claude_result.get("trending_themes", [])

    print(f"\nü§ñ AI TOPIC GENERATION")
    print("-" * 70)
    print(f"  Topics Generated: {len(topics)}")
    print(f"  Claude API Cost: ${report['cost_breakdown']['claude_api']:.4f}")

    # Top 3 topics
    if topics:
        print(f"\nüéØ TOP 3 TOPIC RECOMMENDATIONS")
        print("-" * 70)
        for i, topic in enumerate(topics[:3], 1):
            priority = topic.get('publishing_priority', 0)
            viral = topic.get('viral_potential', 'Unknown')
            print(f"\n  {i}. {topic.get('title', 'Untitled')}")
            print(f"     Priority: {priority}/10 | Viral Potential: {viral}")
            print(f"     Type: {topic.get('video_type', 'N/A')}")

    # Trending themes
    if themes:
        print(f"\nüìà TRENDING THEMES")
        print("-" * 70)
        for theme in themes:
            print(f"  ‚Ä¢ {theme}")

    # Cost breakdown
    print(f"\nüí∞ COST BREAKDOWN")
    print("-" * 70)
    print(f"  Claude API: ${report['cost_breakdown']['claude_api']:.4f}")
    print(f"  YouTube API Quota: {report['cost_breakdown']['youtube_api_quota']} units")
    print(f"  NewsAPI Requests: {report['cost_breakdown']['newsapi_requests']}")

    # Executive summary
    print(f"\nüìù EXECUTIVE SUMMARY")
    print("-" * 70)
    print(f"  {report['executive_summary']}")

    print("\n" + "=" * 70)
    print("‚úÖ Research automation complete!")
    print("=" * 70 + "\n")


def test_run():
    """Quick test of the full workflow"""
    print("\n" + "=" * 70)
    print("üß™ TESTING FULL WORKFLOW")
    print("=" * 70 + "\n")

    report = run_research()

    print("\nüìä TEST RESULTS:")
    print(f"‚úÖ Sources collected: {len(report['sources_collected'])}/5")
    print(f"‚ùå Sources failed: {len(report['sources_failed'])}")
    print(f"ü§ñ Topics generated: {len(report['claude_result']['topic_recommendations'])}")
    print(f"üí∞ Total cost: ${report['cost_breakdown']['claude_api']:.4f}")

    print("\n‚úÖ Full workflow test complete!")
    return report


# =======================================================================
# COMMAND-LINE INTERFACE
# =======================================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Royal Research Automation - YouTube Content Research System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py                  # Run research, print to console
  python main.py --save           # Run research, save to files
  python main.py --save --quiet   # Run research quietly, save to files
  python main.py --test           # Run quick test
        """
    )

    parser.add_argument(
        "--save",
        action="store_true",
        help="Save report to JSON and HTML files"
    )

    parser.add_argument(
        "--quiet",
        action="store_true",
        help="Suppress console summary output"
    )

    parser.add_argument(
        "--test",
        action="store_true",
        help="Run test workflow instead of full run"
    )

    args = parser.parse_args()

    # Print header
    print("\n" + "=" * 70)
    print("ü§ñ ROYAL RESEARCH AUTOMATION SYSTEM")
    print("=" * 70 + "\n")

    start_time = time.time()

    try:
        # Run test or full workflow
        if args.test:
            report = test_run()
        else:
            report = run_research()

        # Save if requested
        if args.save:
            json_path, html_path = save_report(report)
            logger.info("üíæ Reports saved:")
            logger.info(f"   JSON: {json_path}")
            if html_path:
                logger.info(f"   HTML: {html_path}")

        # Print summary (unless quiet)
        if not args.quiet and not args.test:
            print_summary(report)

        # Final runtime
        runtime = time.time() - start_time
        logger.info(f"‚úÖ Complete! Total runtime: {runtime:.1f}s\n")

    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  Process interrupted by user")
        exit(1)

    except Exception as e:
        logger.error(f"‚ùå Research automation failed: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
